apiVersion: v1
kind: Pod
metadata:
  name: llama-rocm-pod
spec:
  volumes:
    - name: model-cache-volume
      emptyDir: {}

  initContainers:
    - name: mcv-init
      image: quay.io/gkm/mcv:latest
      imagePullPolicy: IfNotPresent
      command: ["/bin/bash", "-c"]
      args:
        - |
          set -e
          echo "Running Model Cache Vault (MCV)..."
          /mcv -e -i quay.io/mtahhan/moc-demo:Llama-3.1-8B-Instruct -d /home/vllm/.cache/vllm/

          echo "Extracted cache in /home/vllm/.cache/vllm/..."
          ls -al /home/vllm/.cache/vllm/
      env:
        - name: HIP_VISIBLE_DEVICES
          value: "0"
        - name: HOME
          value: "/home/vllm"
      volumeMounts:
        - name: model-cache-volume
          mountPath: /home/vllm/.cache/vllm/
      securityContext:
        privileged: false
        capabilities:
          add:
            - SYS_ADMIN
        seccompProfile:
          type: Unconfined
      resources:
        limits:
          amd.com/gpu: 1

  containers:
    - name: llama-container
      image: quay.io/mtahhan/qwen-demo:latest
      command: ["./entrypoint-qwen.sh"]
      ports:
        - containerPort: 8000
      env:
        - name: MODEL
          value: "RedHatAI/Llama-3.1-8B-Instruct"
        - name: PORT
          value: "8000"
        - name: MODE
          value: "serve"
        - name: VLLM_USE_COMPILED_ATTENTION
          value: "1"
        - name: VLLM_COMPILED_ATTENTION_BACKEND
          value: "1"
        - name:  VLLM_USE_V1
          value: "1"
        - name:  HUGGING_FACE_HUB_TOKEN
          value: "XXX" #### TODO Populate before starting pod.
      volumeMounts:
        - name: model-cache-volume
          mountPath: /home/vllm/.cache/vllm/
      resources:
        limits:
          amd.com/gpu: 1
      securityContext:
        privileged: true

  restartPolicy: Never
